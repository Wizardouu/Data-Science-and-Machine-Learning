************************************************* explained code **************************************************************************************************************
import numpy as np
from collections import defaultdict

class NaiveBayes:
    def __init__(self):
        self.prior_probabilities = {}
        self.likelihoods = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
        self.classes = []

    def fit(self, X, y):
        print("Fitting the Naive Bayes model...")
        # Store the unique classes
        self.classes = np.unique(y)
        print(f"Classes: {self.classes}")

        # Calculate prior probabilities
        self.prior_probabilities = self.calculate_prior(y)
        print("Prior probabilities:")
        for cls, prob in self.prior_probabilities.items():
            print(f"  P({cls}): {prob:.4f}")

        # Calculate conditional probabilities (likelihoods)
        self.likelihoods = self.calculate_likelihood(X, y)
        print("Likelihoods:")
        for cls in self.classes:
            print(f"  Class: {cls}")
            for feature_idx, feature_likelihoods in self.likelihoods[cls].items():
                print(f"    Feature {feature_idx}:")
                for feature_value, likelihood in feature_likelihoods.items():
                    print(f"      P({feature_value} | {cls}): {likelihood:.4f}")

    def calculate_prior(self, y):
        # caluclate probalility of ( labels or classes or y ) ex: P(yes) = 0.6 , P(NO) = 0.4
        classes, counts = np.unique(y, return_counts=True)
        total_samples = len(y)
        prior_probabilities = {}
        for cls, count in zip(classes, counts):
            prior_probabilities[cls] = count / total_samples
        return prior_probabilities
    # prior_probabilities: array contain each label and its probability

    def calculate_likelihood(self, X, y):


        # likelihoods: Nested dictionary of likelihoods P(Feature | Class)
        # for every value get its probability with each class (with yes and with no)
        # with NO : Feature 0:  P(Red | No): 0.4000 , P(Yellow | No): 0.6000
        # with Yes : Feature 0: P(Red | Yes): 0.6000 , P(Yellow | Yes): 0.4000
        likelihoods = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))

        # Loop over each feature
        for i in range(X.shape[1]):
            feature_values = np.unique(X[:, i])

            for feature_value in feature_values:
                for cls in self.classes:
                    # Filter data by class
                    X_class = X[y == cls]
                    likelihoods[cls][i][feature_value] = np.sum(X_class[:, i] == feature_value) / len(X_class)

        return likelihoods
    # made table contain every P( feature_value | label )

    def predict(self, X_new):
        predictions = []

        for x in X_new:
            posteriors = self.calculate_posteriors(x)
            print(f"Posteriors for sample {x}:")
            for cls, posterior in posteriors.items():
                print(f"  P({cls} | {x}): {posterior:.4f}")
            predicted_class = max(posteriors, key=posteriors.get)   # like argmax
            predictions.append(predicted_class)

        return predictions

    def calculate_posteriors(self, x):
        posteriors = {}

        # Calculate posterior probability for each class
        for cls in self.classes:
            posterior = self.prior_probabilities[cls]  # Start with the prior probability

            # Multiply by the likelihood of each feature
            for i, feature_value in enumerate(x):
                if feature_value in self.likelihoods[cls][i]:
                    posterior *= self.likelihoods[cls][i][feature_value]
                else:
                    # If the feature value was not seen during training, multiply by a small value
                    posterior *= 1e-6  # Smoothing factor to avoid zero probability

            posteriors[cls] = posterior

        return posteriors


# Dataset (same as before)
data = np.array([
    ['Red', 'Sports', 'Domestic', 'Yes'],
    ['Red', 'Sports', 'Domestic', 'No'],
    ['Red', 'Sports', 'Domestic', 'Yes'],
    ['Yellow', 'Sports', 'Domestic', 'No'],
    ['Yellow', 'Sports', 'Imported', 'Yes'],
    ['Yellow', 'SUV', 'Imported', 'No'],
    ['Yellow', 'SUV', 'Imported', 'Yes'],
    ['Yellow', 'SUV', 'Domestic', 'No'],
    ['Red', 'SUV', 'Imported', 'No'],
    ['Red', 'Sports', 'Imported', 'Yes']
])

# Features and target
X = data[:, :-1]  # Features: Color, Type, Origin
y = data[:, -1]   # Target: Stolen (Yes/No)

# Initialize and fit the model
nb = NaiveBayes()
nb.fit(X, y)

# New sample(s) to classify
X_new = np.array([['Red', 'SUV', 'Domestic']])

# Predict the class for the new sample(s)
predictions = nb.predict(X_new)

print("Predicted Class:", predictions[0])

************************************************************************************* dense code ******************************************************************************************************

import numpy as np
from collections import defaultdict


class NaiveBayes:
    def __init__(self):
        self.prior_probabilities = {}
        self.likelihoods = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))
        self.classes = []

    def fit(self, X, y):
        # Store the unique classes
        self.classes = np.unique(y)

        # Calculate prior probabilities
        self.prior_probabilities = self.calculate_prior(y)

        # Calculate conditional probabilities (likelihoods)
        self.likelihoods = self.calculate_likelihood(X, y)

    def calculate_prior(self, y):
        classes, counts = np.unique(y, return_counts=True)
        total_samples = len(y)
        prior_probabilities = {}
        for cls, count in zip(classes, counts):
            prior_probabilities[cls] = count / total_samples
        return prior_probabilities

    def calculate_likelihood(self, X, y):

        likelihoods = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))

        # Loop over each feature
        for i in range(X.shape[1]):
            feature_values = np.unique(X[:, i])

            for feature_value in feature_values:
                for cls in self.classes:
                    # Filter data by class
                    X_class = X[y == cls]
                    likelihoods[cls][i][feature_value] = np.sum(X_class[:, i] == feature_value) / len(X_class)

        return likelihoods

    def predict(self, X_new):
        predictions = []

        for x in X_new:
            posteriors = self.calculate_posteriors(x)
            predicted_class = max(posteriors, key=posteriors.get)
            predictions.append(predicted_class)

        return predictions

    def calculate_posteriors(self, x):

        posteriors = {}

        # Calculate posterior probability for each class
        for cls in self.classes:
            posterior = self.prior_probabilities[cls]  # Start with the prior probability

            # Multiply by the likelihood of each feature
            for i, feature_value in enumerate(x):
                if feature_value in self.likelihoods[cls][i]:
                    posterior *= self.likelihoods[cls][i][feature_value]
                else:
                    # If the feature value was not seen during training, multiply by a small value
                    posterior *= 1e-6  # Smoothing factor to avoid zero probability

            posteriors[cls] = posterior

        return posteriors


# Example usage with the Iris dataset
if __name__ == "__main__":
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_iris

    # Load the Iris dataset
    data = load_iris()
    X, y = data.data, data.target

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize the decision tree
    nb = NaiveBayes()
    nb.fit(X_train, y_train)

    # Predict the labels for the test set
    predictions = nb.predict(X_test)
    # Calculate and print the accuracy
    accuracy = np.sum(predictions == y_test) / len(y_test)
    print(f"Accuracy: {accuracy:.3f}")




